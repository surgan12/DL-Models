{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def my_weight_init(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        init.xavier_uniform(m.weight.data) #initialising the model with random params \n",
    "        init.constant(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.initial_features=512\n",
    "        \n",
    "        self.n_first_layer_inputs=3*3*512\n",
    "        \n",
    "        self.fc1=nn.Linear(100,self.n_first_layer_inputs)\n",
    "        self.bn1=nn.BatchNorm2d(self.initial_features)\n",
    "        \n",
    "        self.deconv1=nn.ConvTranspose2d(self.initial_features,self.initial_features//2,kernel_size=3,stride=2,padding=0,output_padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(self.initial_features//2)\n",
    "        self.deconv2=nn.ConvTranspose2d(self.initial_features//2,self.initial_features//4,kernel_size=5,stride=2,padding=2,output_padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.initial_features//4)\n",
    "        self.deconv3=nn.ConvTranspose2d(self.initial_features//4,1,kernel_size=5,stride=2,padding=2,output_padding=1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            my_weight_init(m)\n",
    "            \n",
    "        \n",
    "    def forward(self,x):\n",
    "            x=self.fc1(x)\n",
    "            x=x.view(-1,512,3,3)\n",
    "            x=F.relu(self.bn1(x))\n",
    "            x=F.relu(self.deconv1(x))\n",
    "            x=self.bn2(x)\n",
    "            x=F.relu(self.deconv2(x))\n",
    "            x=self.bn3(x)\n",
    "            x=F.relu(self.deconv3(x))\n",
    "            x=F.tanh(x)\n",
    "            return x\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "            \n",
    "            super().__init__()\n",
    "            self.conv1=nn.Conv2d(1,64,kernel_size=5,stride=2,padding=2) #14x14x64\n",
    "            self.bn1=nn.BatchNorm2d(64)\n",
    "            self.conv2=nn.Conv2d(64,128,kernel_size=5,stride=2,padding=2) #7x7x128\n",
    "            self.bn2=nn.BatchNorm2d(128)\n",
    "            self.conv3=nn.Conv2d(128,256,kernel_size=5,stride=2,padding=2) #4x4x256\n",
    "            self.bn3=nn.BatchNorm2d(256)\n",
    "            self.fc4=nn.Linear(4*4*256,1)\n",
    "            \n",
    "            for m in self.modules():\n",
    "                my_weight_init(m)\n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        x=F.relu(self.conv1(input))\n",
    "        x=self.bn1(x)\n",
    "        x=F.relu(self.conv2(x))\n",
    "        x=self.bn2(x)\n",
    "        x=F.relu(self.conv3(x))\n",
    "        x=self.bn3(x)\n",
    "        x=x.view(-1,4*4*256)\n",
    "        x=self.fc4(x)\n",
    "        x=F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generator_output(G, fixed_z, img_str, title):\n",
    "    n_images = fixed_z.size()[0]\n",
    "    n_rows = np.sqrt(n_images).astype(np.int32)\n",
    "    n_cols = np.sqrt(n_images).astype(np.int32)\n",
    "    \n",
    "    z_ = fixed_z\n",
    "    samples = G(z_)\n",
    "    samples = samples.data.numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(5,5), sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples):\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box-forced')\n",
    "        ax.imshow(img.reshape((28,28)), cmap='Greys_r', aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle(title)\n",
    "    plt.savefig(img_str)\n",
    "    plt.close(fig)\n",
    "\n",
    "assets_dir = './assets/'\n",
    "if not os.path.isdir(assets_dir):\n",
    "    os.mkdir(assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../Data_sets/MNIST_data', train=True, download=True, transform=transform),\n",
    "    batch_size=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "image_width = 28\n",
    "image_height = 28\n",
    "image_channels = 1\n",
    "x_size = image_channels\n",
    "z_size = 100\n",
    "# n_hidden = 128\n",
    "# n_classes = 10\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "alpha = 0.2\n",
    "beta1 = 0.5\n",
    "print_every = 50\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "fixed_z = torch.Tensor(25, z_size).uniform_(-1, 1)\n",
    "BCE_loss = torch.nn.BCELoss()\n",
    "G_opt = torch.optim.Adam( G.parameters(), lr=learning_rate, betas=[0.5, 0.999] )\n",
    "D_opt = torch.optim.Adam( D.parameters(), lr=learning_rate, betas=[0.5, 0.999] )\n",
    "step=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surgan/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/surgan/.local/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/surgan/.local/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30... Discriminator Loss: 0.0102... Generator Loss: 7.4886\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-12986561649a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mG_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBCE_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mG_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mG_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "for e in range(10):\n",
    "    for x,_ in train_loader:\n",
    "        step+=1\n",
    "        curr_batch_size=x.size()[0]\n",
    "        \n",
    "        y_real=torch.ones(curr_batch_size)\n",
    "        y_fake=torch.zeros(curr_batch_size)\n",
    "        \n",
    "        D_result_real=D(x)\n",
    "        D_loss_real = BCE_loss(D_result_real, y_real)\n",
    "        \n",
    "        #generating the input for getting images using gan\n",
    "        z1_ = torch.Tensor(curr_batch_size, 100).uniform_(-1, 1)\n",
    "        x_fake=G(z1_)\n",
    "        D_result_fake= D(x_fake)\n",
    "        D_loss_fake = BCE_loss(D_result_fake,y_fake)\n",
    "        \n",
    "        D_loss=D_loss_fake + D_loss_real\n",
    "        \n",
    "        D.zero_grad()\n",
    "        \n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "        \n",
    "        #training the generator\n",
    "        \n",
    "        z2=torch.Tensor(curr_batch_size,100).uniform_(-1,1)\n",
    "        y_=torch.ones(curr_batch_size)\n",
    "        G_res=G(z2)\n",
    "        D_G=D(G_res)\n",
    "        G_loss=BCE_loss(D_G,y_)\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_opt.step()\n",
    "        if step % print_every == 0:\n",
    "            losses.append((D_loss.data[0], G_loss.data[0]))\n",
    "\n",
    "            print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "                \"Discriminator Loss: {:.4f}...\".format(D_loss.data[0]),\n",
    "                \"Generator Loss: {:.4f}\".format(G_loss.data[0])) \n",
    "    # Sample from generator as we're training for viewing afterwards\n",
    "    image_fn = './assets/epoch_{:d}_pytorch.png'.format(e)\n",
    "    image_title = 'epoch {:d}'.format(e)\n",
    "    save_generator_output(G, fixed_z, image_fn, image_title)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
