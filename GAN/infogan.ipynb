{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Front_End(nn.Module):\n",
    "    '''front end of disc and q remains same only at the end that probability changes'''\n",
    "    def __init__(self):\n",
    "        super(Front_End,self).__init__()\n",
    "        self.main=nn.Sequential(nn.Conv2d(1,64,4,2,1),\n",
    "        nn.LeakyReLU(0.1,inplace=True),\n",
    "        nn.Conv2d(64,128,4,2,1,bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.1,inplace=True),\n",
    "        \n",
    "        nn.Conv2d(128,1024,7,bias=False),\n",
    "        nn.BatchNorm2d(1024),\n",
    "        nn.LeakyReLU(0.1,inplace=True),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        output=self.main(x)\n",
    "        return output\n",
    "\n",
    "class Disc(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Disc,self).__init__()\n",
    "        self.main=nn.Sequential(\n",
    "        nn.Conv2d(1024,1,1),\n",
    "        nn.Sigmoid(),)\n",
    "    def forward(self,x):\n",
    "        output=self.main(x)\n",
    "        output=output.view(-1,1)\n",
    "        return output\n",
    "\n",
    "class Qr(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Qr,self).__init__()\n",
    "        self.conv=nn.Conv2d(1024,128,1)\n",
    "        self.conv_disc=nn.Conv2d(128,10,1)\n",
    "        self.conv_mu=nn.Conv2d(128,2,1)\n",
    "        self.conv_var=nn.Conv2d(128,2,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=self.conv(x)\n",
    "        disc_logits=self.conv_disc(y).squeeze()\n",
    "        mu=self.conv_mu(y).squeeze()\n",
    "        var=self.conv_var(y).squeeze()\n",
    "        \n",
    "        return disc_logits,mu,var\n",
    "\n",
    "\n",
    "class Gen(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Gen,self).__init__()\n",
    "        \n",
    "        self.fc1=nn.Linear(74,1024)\n",
    "        \n",
    "        self.main=nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(1024,128,7,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,1,4,2,1,bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=x.view(-1,74)\n",
    "        x=self.fc1(x)\n",
    "        x=x.view(-1,1024,1,1)\n",
    "        output=self.main(x)\n",
    "        return output\n",
    "    \n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_gaussian:\n",
    "\n",
    "  def __call__(self, x, mu, var):\n",
    "\n",
    "    logli = -0.5*(var.mul(2*np.pi)+1e-6).log() - \\\n",
    "            (x-mu).pow(2).div(var.mul(2.0)+1e-6)\n",
    "    \n",
    "    return logli.sum(1).mean().mul(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=15\n",
    "idx = np.random.randint(10,size=bs)\n",
    "c = np.zeros((bs, 10))\n",
    "c[range(bs),idx] = 1.0\n",
    "\n",
    "dis_c=torch.Tensor(c)\n",
    "con_c=torch.Tensor(bs,2).uniform_(-1.0,1.0)\n",
    "noise=torch.Tensor(bs,62).uniform_(-1.0,1.0)\n",
    "z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "batch_size=bs\n",
    "# data_loader normalize [0, 1] ==> [-1, 1]\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('../Data_sets/MNIST_data', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qr(\n",
       "  (conv): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv_disc): Conv2d(128, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv_mu): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv_var): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterionD=nn.BCELoss()\n",
    "criterionQ_dis=nn.MSELoss()\n",
    "criterionQ_con=log_gaussian()\n",
    "FE=Front_End().cuda()\n",
    "G=Gen().cuda()\n",
    "D=Disc().cuda()\n",
    "Q=Qr().cuda()\n",
    "\n",
    "FE.apply(weights_init)\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "Q.apply(weights_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          0.        ]\n",
      " [-0.85714286  0.        ]\n",
      " [-0.71428571  0.        ]\n",
      " [-0.57142857  0.        ]\n",
      " [-0.42857143  0.        ]\n",
      " [-0.28571429  0.        ]\n",
      " [-0.14285714  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.14285714  0.        ]\n",
      " [ 0.28571429  0.        ]\n",
      " [ 0.42857143  0.        ]\n",
      " [ 0.57142857  0.        ]\n",
      " [ 0.71428571  0.        ]\n",
      " [ 0.85714286  0.        ]\n",
      " [ 1.          0.        ]]\n",
      "[8 7 1 7 3 7 9 7 5 4 6 7 9 6 4]\n",
      "Epoch/Iter:0/0, Dloss: 1.4552903175354004, Gloss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infero/.local/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch/Iter:0/15, Dloss: 1.8851802349090576, Gloss: nan\n",
      "Epoch/Iter:0/30, Dloss: 3.1967902183532715, Gloss: 2.4683454036712646\n",
      "Epoch/Iter:0/45, Dloss: 4.352138519287109, Gloss: nan\n",
      "Epoch/Iter:0/60, Dloss: 5.97489595413208, Gloss: nan\n",
      "Epoch/Iter:0/75, Dloss: 6.385566711425781, Gloss: nan\n",
      "Epoch/Iter:0/90, Dloss: 8.77228832244873, Gloss: nan\n",
      "Epoch/Iter:0/105, Dloss: 8.574454307556152, Gloss: nan\n",
      "Epoch/Iter:0/120, Dloss: 9.012786865234375, Gloss: 2.4266304969787598\n",
      "Epoch/Iter:0/135, Dloss: 11.107748985290527, Gloss: nan\n",
      "Epoch/Iter:0/150, Dloss: 11.950663566589355, Gloss: nan\n",
      "Epoch/Iter:0/165, Dloss: 9.981861114501953, Gloss: nan\n",
      "Epoch/Iter:0/180, Dloss: 13.162750244140625, Gloss: nan\n",
      "Epoch/Iter:0/195, Dloss: 10.980905532836914, Gloss: nan\n",
      "Epoch/Iter:0/210, Dloss: 12.234779357910156, Gloss: nan\n",
      "Epoch/Iter:0/225, Dloss: 13.347461700439453, Gloss: nan\n",
      "Epoch/Iter:0/240, Dloss: 19.13930320739746, Gloss: nan\n",
      "Epoch/Iter:0/255, Dloss: 16.24195671081543, Gloss: nan\n",
      "Epoch/Iter:0/270, Dloss: 13.575523376464844, Gloss: 1.9718151092529297\n",
      "Epoch/Iter:0/285, Dloss: 16.210845947265625, Gloss: nan\n",
      "Epoch/Iter:0/300, Dloss: 17.203834533691406, Gloss: nan\n",
      "Epoch/Iter:0/315, Dloss: 17.51462173461914, Gloss: nan\n",
      "Epoch/Iter:0/330, Dloss: 17.720121383666992, Gloss: nan\n",
      "Epoch/Iter:0/345, Dloss: 18.246807098388672, Gloss: nan\n",
      "Epoch/Iter:0/360, Dloss: 22.28060531616211, Gloss: nan\n",
      "Epoch/Iter:0/375, Dloss: 19.99565315246582, Gloss: nan\n",
      "Epoch/Iter:0/390, Dloss: 20.49237823486328, Gloss: nan\n",
      "Epoch/Iter:0/405, Dloss: 22.226539611816406, Gloss: nan\n",
      "Epoch/Iter:0/420, Dloss: 19.20769691467285, Gloss: nan\n",
      "Epoch/Iter:0/435, Dloss: 21.767505645751953, Gloss: nan\n",
      "Epoch/Iter:0/450, Dloss: 20.74112892150879, Gloss: nan\n",
      "Epoch/Iter:0/465, Dloss: 18.642845153808594, Gloss: 2.11651873588562\n",
      "Epoch/Iter:0/480, Dloss: 20.035295486450195, Gloss: nan\n",
      "Epoch/Iter:0/495, Dloss: 23.020854949951172, Gloss: 2.47711443901062\n",
      "Epoch/Iter:0/510, Dloss: 17.938373565673828, Gloss: 2.219414472579956\n",
      "Epoch/Iter:0/525, Dloss: 25.483901977539062, Gloss: 2.217890977859497\n",
      "Epoch/Iter:0/540, Dloss: 23.49472427368164, Gloss: 2.29103422164917\n",
      "Epoch/Iter:0/555, Dloss: 22.62764549255371, Gloss: 2.520064353942871\n",
      "Epoch/Iter:0/570, Dloss: 23.984514236450195, Gloss: nan\n",
      "Epoch/Iter:0/585, Dloss: 23.145435333251953, Gloss: 2.0805914402008057\n",
      "Epoch/Iter:0/600, Dloss: 21.279102325439453, Gloss: 2.594043731689453\n",
      "Epoch/Iter:0/615, Dloss: 22.655946731567383, Gloss: 2.4550323486328125\n",
      "Epoch/Iter:0/630, Dloss: 23.506771087646484, Gloss: 2.7299036979675293\n",
      "Epoch/Iter:0/645, Dloss: 22.46016502380371, Gloss: 2.594650983810425\n",
      "Epoch/Iter:0/660, Dloss: 22.83297348022461, Gloss: 2.6462817192077637\n",
      "Epoch/Iter:0/675, Dloss: 24.5635929107666, Gloss: 2.867807149887085\n",
      "Epoch/Iter:0/690, Dloss: 21.31721305847168, Gloss: 2.857539653778076\n",
      "Epoch/Iter:0/705, Dloss: 23.284759521484375, Gloss: 2.9654481410980225\n",
      "Epoch/Iter:0/720, Dloss: 24.996337890625, Gloss: 2.835373878479004\n",
      "Epoch/Iter:0/735, Dloss: 25.304027557373047, Gloss: 2.597409248352051\n",
      "Epoch/Iter:0/750, Dloss: 25.420103073120117, Gloss: 2.786407947540283\n",
      "Epoch/Iter:0/765, Dloss: 26.65548324584961, Gloss: 2.8742775917053223\n",
      "Epoch/Iter:0/780, Dloss: 27.63102149963379, Gloss: 2.5199248790740967\n",
      "Epoch/Iter:0/795, Dloss: 23.706758499145508, Gloss: 2.905691385269165\n",
      "Epoch/Iter:0/810, Dloss: 24.134597778320312, Gloss: 2.8945255279541016\n",
      "Epoch/Iter:0/825, Dloss: 26.851778030395508, Gloss: 2.750049591064453\n",
      "Epoch/Iter:0/840, Dloss: 26.29796600341797, Gloss: 2.7700822353363037\n",
      "Epoch/Iter:0/855, Dloss: 22.274507522583008, Gloss: 2.98354172706604\n",
      "Epoch/Iter:0/870, Dloss: 23.163421630859375, Gloss: 2.2740659713745117\n",
      "Epoch/Iter:0/885, Dloss: 26.211854934692383, Gloss: 2.833381414413452\n",
      "Epoch/Iter:0/900, Dloss: 26.61867904663086, Gloss: 2.473903179168701\n",
      "Epoch/Iter:0/915, Dloss: 26.453359603881836, Gloss: 2.6639223098754883\n",
      "Epoch/Iter:0/930, Dloss: 22.581579208374023, Gloss: 2.3547401428222656\n",
      "Epoch/Iter:0/945, Dloss: 25.725236892700195, Gloss: 2.8062214851379395\n",
      "Epoch/Iter:0/960, Dloss: 26.71314811706543, Gloss: 2.6805450916290283\n",
      "Epoch/Iter:0/975, Dloss: 27.63102149963379, Gloss: 2.72237491607666\n"
     ]
    }
   ],
   "source": [
    "optimD = optim.Adam([{'params':FE.parameters()}, {'params':D.parameters()}], lr=0.0002, betas=(0.5, 0.99))\n",
    "optimG = optim.Adam([{'params':G.parameters()}, {'params':Q.parameters()}], lr=0.001, betas=(0.5, 0.99))\n",
    "\n",
    "x = np.linspace(-1, 1, 15).reshape(1, -1)\n",
    "x = x.reshape(-1, 1)\n",
    "c1 = np.hstack([x, np.zeros_like(x)])\n",
    "print(c1)\n",
    "c2 = np.hstack([np.zeros_like(x), x])\n",
    "\n",
    "idx = np.random.randint(10,size=bs)\n",
    "one_hot = np.zeros((bs, 10))\n",
    "one_hot[range(bs), idx] = 1\n",
    "print(idx)\n",
    "fix_noise = torch.Tensor(bs, 62).uniform_(-1, 1)\n",
    "for epoch in range(40):\n",
    "    for num_iters,data in enumerate(train_loader,0):\n",
    "        real_x,labels=data\n",
    "        idx = np.random.randint(10,size=bs)\n",
    "        c = np.zeros((bs, 10))\n",
    "        c[range(bs),idx] = 1.0\n",
    "        real_x=Variable(real_x.cuda())\n",
    "\n",
    "        dis_c=torch.Tensor(c)\n",
    "        con_c=torch.Tensor(bs,2).uniform_(-1.0,1.0)\n",
    "        noise=torch.Tensor(bs,62).uniform_(-1.0,1.0)\n",
    "        fe_out1=FE(real_x)\n",
    "        probs_real=D(fe_out1)\n",
    "        real_lab=torch.ones(bs)\n",
    "        real_lab=Variable(real_lab.cuda())\n",
    "        loss_real=criterionD(probs_real,real_lab)\n",
    "        loss_real.backward()\n",
    "        z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "        z=Variable(z.cuda())\n",
    "        fake_x=G(z)\n",
    "        fe_out2=FE(fake_x.detach())\n",
    "        probs_fake=D(fe_out2)\n",
    "        fake_lab=torch.zeros(bs)\n",
    "        probs_fake=Variable(probs_fake.cuda(),requires_grad=True)\n",
    "        fake_lab=Variable(fake_lab.cuda())\n",
    "        loss_fake=criterionD(probs_fake,fake_lab)                                   \n",
    "        loss_fake.backward() \n",
    "        D_loss=loss_real+loss_fake\n",
    "        optimD.step()\n",
    "        \n",
    "        #G & Q part\n",
    "        optimG.zero_grad()\n",
    "        fe_out=FE(fake_x)\n",
    "        probs_fake=D(fe_out)\n",
    "        label=torch.ones(bs)\n",
    "        label=Variable(label.cuda())\n",
    "        probs_fake=Variable(probs_fake.cuda())\n",
    "        class_ = torch.LongTensor(idx)\n",
    "        recon_loss=criterionD(probs_fake,label)\n",
    "        \n",
    "        q_logits , q_mu, q_var=Q(fe_out)\n",
    "        q_logits=Variable(q_logits.cuda())\n",
    "        class_=Variable(class_.cuda())\n",
    "        one = np.zeros((bs, 10))\n",
    "        for i in range(len(class_)):\n",
    "            one[i,class_[i]]=1\n",
    "        one=torch.from_numpy(one)\n",
    "        one=one.type(torch.FloatTensor)\n",
    "        q_logits=q_logits.type(torch.FloatTensor)\n",
    "        class_=class_.type(torch.FloatTensor)\n",
    "        dis_loss = criterionQ_dis(q_logits,one)\n",
    "        \n",
    "        con_c=Variable(con_c.cuda())\n",
    "        con_loss = criterionQ_con(con_c, q_mu, q_var)\n",
    "        recon_loss=Variable(recon_loss.cuda())\n",
    "        dis_loss=Variable(dis_loss.cuda())\n",
    "        \n",
    "        G_loss= recon_loss + dis_loss + con_loss\n",
    "        G_loss.backward()\n",
    "        optimG.step()\n",
    "        if num_iters %15 == 0:\n",
    "\n",
    "          print('Epoch/Iter:{0}/{1}, Dloss: {2}, Gloss: {3}'.format(\n",
    "            epoch, num_iters, D_loss.data.cpu().numpy(),\n",
    "            G_loss.data.cpu().numpy())\n",
    "          )\n",
    "          f=torch.from_numpy(c1)\n",
    "          f=f.type(torch.FloatTensor)\n",
    "          tr=torch.FloatTensor(one_hot)\n",
    "          tr=tr.type(torch.FloatTensor)\n",
    "          z = torch.cat([noise, tr, f], 1).view(-1, 74, 1, 1)\n",
    "          z=Variable(z.cuda())\n",
    "          x_save = G(z)\n",
    "          save_image(x_save.data, './tmp/c1.png', nrow=10)\n",
    "\n",
    "          f2=torch.from_numpy(c1)\n",
    "          f2=f2.type(torch.FloatTensor)\n",
    "          z = torch.cat([fix_noise, tr, f2], 1).view(-1, 74, 1, 1)\n",
    "          z=Variable(z.cuda())\n",
    "          x_save = G(z)\n",
    "          save_image(x_save.data, './tmp/c2.png', nrow=10)\n",
    "        \n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
